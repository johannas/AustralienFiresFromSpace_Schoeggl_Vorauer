{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sat May 23 15:34:02 2020\n",
    "\n",
    "@author: Edgar Alfred Johnson as Johanna Schoeggl\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np # linear algebra\n",
    "import os # accessing directory structure\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "from pandas import Timestamp\n",
    "import csv\n",
    "from helferlein_thomas import files\n",
    "#from firesfromSpace import date_seperating \n",
    "from scipy.stats import poisson\n",
    "from scipy.stats import expon\n",
    "import folium\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "#alle nasadaten von 2000 bis 2020\n",
    "\n",
    "# pfad zum ordner wo die csv datein liegen\n",
    "path = r'D:\\Dokumente\\PythonStuff\\fires-from-space-australia-and-new-zeland\\allnasa'\n",
    "\n",
    "# reinladen der einzelnen datens채tze und speichern als dataframe\n",
    "m6_archive  = pd.read_csv(path+os.sep+files(path)[0]).drop('type', 1)\n",
    "v1_archive  = pd.read_csv(path+os.sep+files(path)[1]).drop('type', 1)\n",
    "m6_nrt      = pd.read_csv(path+os.sep+files(path)[2])\n",
    "v1_nrt      = pd.read_csv(path+os.sep+files(path)[3])\n",
    "\n",
    "#damit die verteilungen sinn ergebe sollte man vermutlich die unwichtigen spalten droppen!  \n",
    "v1a_renamed = v1_archive.rename(columns={'bright_ti4':'brightnessV'})\n",
    "v1n_renamed = v1_nrt.rename(columns={'bright_ti4':'brightnessV'})\n",
    "\n",
    "V1a = v1a_renamed.copy()\n",
    "V1n = v1n_renamed.copy()\n",
    "M6a = m6_archive.copy()\n",
    "M6n = m6_nrt.copy()\n",
    "\n",
    "M6a.drop(['acq_time', 'satellite', 'instrument', 'version', 'daynight'], 1 )\n",
    "\n",
    "\n",
    "\n",
    "M6a['acq_date'] = pd.to_datetime(M6a['acq_date']) - pd.to_timedelta(7, unit='d')\n",
    "\n",
    "dfM6A = M6a.groupby([pd.Grouper(key='acq_date', freq='W-MON')])['brightness', 'bright_t31'].mean().reset_index().sort_values('acq_date')\n",
    "\n",
    "#plt.plot(np.array(df.brightness))\n",
    "plt.figure()\n",
    "plt.plot(M6a.brightness)\n",
    "plt.plot(M6a.bright_t31)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(dfM6A.brightness)\n",
    "plt.plot(dfM6A.bright_t31)\n",
    "\n",
    "\n",
    "V1a['acq_date'] = pd.to_datetime(V1a['acq_date']) - pd.to_timedelta(7, unit='d')\n",
    "\n",
    "df2 = V1a.groupby([pd.Grouper(key='acq_date', freq='W-MON')])['brightnessV', 'bright_ti5'].mean().reset_index().sort_values('acq_date')\n",
    "\n",
    "plt.plot(df2.brightnessV)\n",
    "plt.plot(df2.bright_ti5)  \n",
    "         \n",
    "plt.axes()\n",
    "\n",
    "V1a.keys()\n",
    "m6_nrt['acq_date'] = pd.to_datetime(m6_nrt['acq_date'])-pd.to_timedelta(7, unit='d')\n",
    "m6_archive['acq_date'] = pd.to_datetime(m6_archive['acq_date'])-pd.to_timedelta(7, unit='d')\n",
    "def date_seperating(df, sel): \n",
    "    '''splits dataframes into weeks, months, years'''\n",
    "    out = []\n",
    "    if 'D' in sel:\n",
    "        out.append([g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'D'))])\n",
    "    if 'W' in sel:\n",
    "        out.append([g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'W'))])\n",
    "    if 'M' in sel:\n",
    "        out.append([g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'M'))])\n",
    "    if 'Y' in sel:\n",
    "        out.append([g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'Y'))])\n",
    "    return out    \n",
    "    # return ([g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'D'))],\n",
    "    #         [g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'W'))], \n",
    "    #         [g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'M'))],\n",
    "    #         [g for n, g in df.set_index('acq_date').groupby(pd.Grouper(freq = 'Y'))])\n",
    "\n",
    "weeks, month, year = date_seperating(m6_archive)\n",
    "week0 = weeks[0]\n",
    "\n",
    "day_test =[g for n, g in m6_nrt.set_index('acq_date').groupby(pd.Grouper(freq = 'D'))]\n",
    "daym =[g for n, g in m6_archive.set_index('acq_date').groupby(pd.Grouper(freq = 'D'))]\n",
    "# m6_apoisson = poisson.rvs(mu=3, size=df)\n",
    "weeks_test =[g for n, g in m6_archive.set_index('acq_date').groupby(pd.Grouper(freq = 'W'))]\n",
    "years_test =[g for n, g in m6_archive.set_index('acq_date').groupby(pd.Grouper(freq = 'Y'))]\n",
    "\n",
    "#map(lambda, ['longitude', 'latitude', 'longitude', 'latitude']) = \n",
    "\n",
    "#Create a map\n",
    "f = folium.Figure(width=1000, height=500)\n",
    "center_lat = -24.003249 \n",
    "center_long = 133.737310\n",
    "m = folium.Map(location=[center_lat,center_long], control_scale=True, zoom_start=4,width=750, height=500,zoom_control=True).add_to(f)\n",
    "for i in range(0,week0.shape[0]):    \n",
    "    location=[week0.iloc[i]['latitude'], week0.iloc[i]['longitude']]\n",
    "    folium.CircleMarker(location,radius=1,color='red').add_to(m)\n",
    "\n",
    "display(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#so man muss die parts durch alle i aus week0 ersetzen (alle koordinaten) und dann?\n",
    "\n",
    "# w = shapefile.Writer(shapefile.POLYGON)\n",
    " \n",
    "# w.poly(parts=[[[1,5],[5,5],[5,1],[3,3],[1,1]]])\n",
    "#  w.field('FIRST_FLD','C','40')\n",
    "#  w.field('SECOND_FLD','C','40')\n",
    "#  w.record('First','Polygon')\n",
    "#  w.save('shapefiles/test/polygon')\n",
    "\n",
    "\n",
    "#plot f체r die vektornorm\n",
    "# for i in range(len(m6_archive)):\n",
    "#     for j in range(i, len(m6_archive)):\n",
    "#         np.array([m6_archive['latitude'][i], m6_archive['longitude'][i]]) -\n",
    "#         np.array([m6_archive['latitude'][j], m6_archive['longitude'][j]])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#datum + koordinate : feuereuabreitung koordinaten abstand\n",
    "\n",
    "days = date_seperating(m6_archive, 'D')\n",
    "\n",
    "# mathode k-means-clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = np.array(days[0][0][['latitude','longitude']])\n",
    "Y = np.array(days[0][1][['latitude','longitude']])\n",
    "\n",
    "kmeans = KMeans(n_clusters=15).fit_predict(X)     \n",
    "kmeans.predict(Y)\n",
    "kmeans.cluster_centers_\n",
    "#kmeans.fit(Y)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[0][0].longitude, X[0][0].latitude, c=kmeans)\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "# # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "# h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# #!!!JS???!!!  Plot the decision boundary. For that, we will assign a color to each cluster !!!JS???!!! \n",
    "# x_min, x_max = kmeans[0 , 1].min() - 1, kmeans[0 , 1].max() + 1\n",
    "# y_min, y_max = kmeans[0 , 1].min() - 1, kmeans[0 , 1].max() + 1\n",
    "# xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# # Obtain labels for each point in mesh. Use last trained model.\n",
    "# Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# # Put the result into a color plot\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# plt.figure(1)\n",
    "# plt.clf()\n",
    "# plt.imshow(Z, interpolation='nearest',\n",
    "#            extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "#            cmap=plt.cm.Paired,\n",
    "#            aspect='auto', origin='lower')\n",
    "\n",
    "# plt.plot(kmeans[0 , 1], kmeans[0 ,1], 'k.', markersize=2)\n",
    "# # Plot the centroids as a white X\n",
    "# centroids = kmeans.cluster_centers_\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "#             marker='x', s=169, linewidths=3,\n",
    "#             color='w', zorder=10)\n",
    "# plt.title('K-means clustering fires')\n",
    "# plt.xlim(x_min, x_max)\n",
    "# plt.ylim(y_min, y_max)\n",
    "# plt.xticks(())\n",
    "# plt.yticks(())\n",
    "# plt.show()\n",
    "\n",
    " #%%\n",
    "w0 = weeks_test[0]\n",
    "plt.plot(w0.brightness)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "m6_archive['acq_date'] = pd.to_datetime(m6_archive['acq_date'])\n",
    "monthlmean = m6_archive.groupby([pd.Grouper(key='acq_date', freq='M')])['brightness'].mean().reset_index().sort_values('acq_date')\n",
    "sns.scatter(monthlmean)\n",
    "#f체r dfs hw = [i.max() for i in weeks_test['brightness'] \n",
    "\n",
    "\n",
    "\n",
    "#f체r listen\n",
    "hw = [i.brightness.mean() for i in weeks_test]\n",
    "plt.plot(hw)\n",
    "\n",
    "\n",
    "\n",
    "#hottest days in all years\n",
    "hd_m6a_b1 = (m6_archive.brightness.max() , m6_archive.acq_date)\n",
    "hd_m6a_b2 = (m6_archive.bright_t31.max() , m6_archive.acq_date)\n",
    "hd_v1a_b1 = (v1a_renamed.brightnessV.max() , v1a_renamed.acq_date)\n",
    "hd_v1a_b2 = (v1a_renamed.bright_ti5.max() , v1a_renamed.acq_date)\n",
    "\n",
    "print([hd_m6a_b1, hd_m6a_b2, hd_v1a_b1, hd_v1a_b2])\n",
    "\n",
    "plt.plot([hd_m6a_b1, hd_m6a_b2, hd_v1a_b1, hd_v1a_b2])\n",
    "\n",
    "print(hd_m6a_b1)\n",
    "\n",
    " #%%\n",
    "w0 = weeks_test[0]\n",
    "plt.plot(w0.brightness)\n",
    "\n",
    " #%%\n",
    "\n",
    "fig, ax = plt.subplots(2, 2)\n",
    "\n",
    "a = ax.ravel()\n",
    "\n",
    "for i, j in enumerate(weeks_test[:4]):\n",
    "   # a[i].plot(j.acq_date, j.brightness)\n",
    "    \n",
    "    a[i].plot(j.set_index('acq_date').brightness)\n",
    "         \n",
    " #%%\n",
    "    \n",
    "    weeks_m, months_m, years_m = wmy(allmodis)\n",
    "weeks_v, months_v, years_v = wmy(allviirs)\n",
    "\n",
    "ym1 = years_m[1].groupby(pd.Grouper(freq = 'M')).mean()\n",
    "ym2 = years_m[2].groupby(pd.Grouper(freq = 'M')).mean()\n",
    "\n",
    "ym = [i.groupby(pd.Grouper(freq = 'M')).mean() for i in years_m]\n",
    "\n",
    "\n",
    "plt.figure('brightness')\n",
    "plt.clf()\n",
    "for i in range(15,20):\n",
    "    plt.plot(ym[i].reset_index().brightness, 'o-', label = 'brightness-{}'.format(i))\n",
    "# plt.plot(ym2.reset_index().brightness, 'o-', label = 'brightness2')\n",
    "plt.legend()\n",
    "\n",
    " #%%\n",
    "\n",
    "plt.figure()\n",
    "sns.scatterplot(m6_archive.latitude, m6_archive.longitude, hue= m6_archive.brightness)\n",
    "\n",
    "plt.plot(m6_archive.groupby(['acq_date']).brightness.mean())\n",
    "plt.plot(m6_archive.groupby(['acq_date']).bright_t31.mean())\n",
    "plt.plot(m6_archive.groupby(['acq_date'])frp.mean())\n",
    "\n",
    "plt.figure()\n",
    "px.bar(m6_archive, x = 'frp', y = 'brightness', orientation = 'v')\n",
    "\n",
    "\n",
    "plt.scatter(m6_archive.X, m6_archive.Y, label = 'm6_archive', c =m6_archive['brightness'], linewidths=2, alpha = .5, cmap = 'jet')\n",
    "plt.scatter(m6_archive.X, m6_archive.Y, label = 'm6_archive', c =m6_archive['brigth_t31'], linewidths=2, alpha = .5, cmap = 'jet')\n",
    "plt.scatter(m6_archive.X, m6_archive.Y, label = 'm6_archive', c =m6_archive['frp'], linewidths=2, alpha = .5, cmap = 'jet')\n",
    "\n",
    "\n",
    "df = m6_archive.groupby([pd.Grouper(key='month', freq='W-MON')])['bright_t31', 'frp'].mean().reset_index().sort_values('acq_date')\n",
    "\n",
    " #%%\n",
    "\n",
    "fig, ax = plt.subplots(2,2, num = 'locations', figsize = (16,10))\n",
    "a = ax.ravel()\n",
    "for i,j in enumerate(weeks_test):\n",
    "#ae = achsenelement  \n",
    "    ae = a[i].scatter(j.X, j.Y, marker = '.', label = weeks_test[i] , c = j.frp)\n",
    "    fig.colorbar(ae, ax = a[i])\n",
    "    a[i].legend()\n",
    "    a[i].grid(True)\n",
    "\n",
    "plt.show()\n",
    " #%%\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "scat = m6_archive(np.random.randn(1000, 4), columns = ['frp', 'brightness', 'bright_t31', 'acq_date'])\n",
    "scatter_matrix(m6_archive[['frp', 'brightness', 'bright_t31', 'acq_date']], alpha = 0.2, figsize = (6, 6), diagonal = 'kde')\n",
    "\n",
    " #%%\n",
    "\n",
    "m6_archive.corr(method ='pearson') \n",
    "\n",
    "\n",
    "m6_archive.corr(method ='kendall') \n",
    " #%%\n",
    "m6_archive.keys()\n",
    "\n",
    "# #plot \n",
    "# fig = plt.figure() \n",
    "# ax = fig.add_subplot(111) \n",
    "# ax.plot(forestfires['temp'], forestfires['FFMC'], ls='', ms=10, mew=2) \n",
    "# ax.plot(forestfires['DMC'], forestfires['DC'], ls='', ms=10, mew=2)\n",
    "# ax.plot(forestfires['temp'], forestfires['ISI'], ls='', ms=10, mew=2) \n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "# sets = px.data.tips()\n",
    "# fig = px.parallel_categories(sets)\n",
    "\n",
    "# fig.show()\n",
    "\n",
    "m6_archive.keys()\n",
    "\n",
    "df_set = px.data.tips()\n",
    "fig = px.parallel_categories(m6_archive, dimensions=['frp', 'brightness', 'confidence'],\n",
    "                color=\"Rain ID\", color_continuous_scale=px.colors.sequential.Inferno,\n",
    "                labels={'rain':'amount', 'ceonfidence':'rc', 'day':'Day/time'})\n",
    "fig.show()\n",
    "######\n",
    " #%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
